# Chat-with-Audio-using-LLM
This is a Streamlit application that helps you chat with your audio file powered by Langchain, ChromaDB, and OpenAI.

<h1 align="center">
  <br>
  <img src="https://github.com/calicartels/LLMTalk/blob/main/_d2855674-b180-4015-95e3-e2cbf462f6fc.jpeg" alt="LLMTalk" width="300">
  <br>
  <a href="https://github.com/calicartels/blind.ai">LLMTalk</a>
  <br>
</h1>

<h4 align="center">An app to help people skip the nuances of wathcing the whole video</h4>

<p align="center">
  <a href="#introduction">Introduction</a> •
  <a href="#installation">Installation</a> •
  <a href="#application-structure">Application Structure</a> •
  <a href="#key-features">Key Features</a> •
  <a href="#how-to-use">How To Use</a> •
  <a href="#license">License</a>
</p>

<p align="center">
<img width = 100% src="https://github.com/calicartels/LLMTalk/blob/main/LLMTalk.png" alt="App">
</p>

## Introduction

The development of tools and technology hasn't resulted in the development of applications that could aid those with visual impairments. With the development of Data Modelling techniques, which can be used to give even basic computers a bit of "intelligence," and the ease of accessibility, this "intelligence" can be extended to our smartphones to aid those who are blind in navigating their surroundings and going about their daily lives. By utilising the power of Deep Learning, which can be made accessible even on low-end devices with a clear User-Interface that would precisely allow them to better grasp the world around, our application seeks to close the gap between them and the visible world.

This app enables the community of blind and visually impaired people to correctly identify objects they come across in everyday life without the need for sighted assistance.

## Installation

Install with pip:

```
$ pip install -r requirements.txt
```

